{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Analysis is a statistical process for estimating the relationships between the dependent variables or criterion variables and one or more independent variables or predictors. Regression analysis is generally used when we deal with a dataset that has the target variable in the form of continuous data. Regression analysis explains the changes in criteria about changes in select predictors. The conditional expectation of the criteria is based on predictors where the average value of the dependent variables is given when the independent variables are changed. Three major uses for regression analysis are determining the strength of predictors, forecasting an effect, and trend forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Regression Techniques\n",
    "Along with the development of the machine learning domain regression analysis techniques have gained popularity as well as developed manifold from just y = mx + c. There are several types of regression techniques, each suited for different types of data and different types of relationships. The main types of regression techniques are:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Polynomial Regression\n",
    "3. Stepwise Regression\n",
    "4. Decision Tree Regression\n",
    "5. Random Forest Regression\n",
    "6. Support Vector Regression\n",
    "7. Ridge Regression\n",
    "8. Lasso Regression\n",
    "9. ElasticNet Regression\n",
    "10. Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Linear regression is used for predictive analysis. Linear regression is a linear approach for modeling the relationship between the criterion or the scalar response and the multiple predictors or explanatory variables. Linear regression focuses on the conditional probability distribution of the response given the values of the predictors. For linear regression, there is a danger of overfitting. The formula for linear regression is:\n",
    "\n",
    "Syntax:\n",
    "\n",
    "y = θx + b\n",
    "\n",
    "where,\n",
    "\n",
    "θ – It is the model weights or parameters\n",
    "b – It is known as the bias.\n",
    "This is the most basic form of regression analysis and is used to model a linear relationship between a single dependent variable and one or more independent variables.\n",
    "\n",
    "Here, a linear regression model is instantiated to fit a linear relationship between input features (X) and target values (y). This code is used for simple demonstration of the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "This is an extension of linear regression and is used to model a non-linear relationship between the dependent variable and independent variables. Here as well syntax remains the same but now in the input variables we include some polynomial or higher degree terms of some already existing features as well. Linear regression was only able to fit a linear model to the data at hand but with polynomial features, we can easily fit some non-linear relationship between the target as well as input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import PolynomialRegression\n",
    "\n",
    "# Create a polynomial regression model\n",
    "model = PolynomialRegression(degree=2)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "```"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGwAAAASCAYAAAC+Tjt8AAAEhElEQVRYCe2Y56teRRCHHxtqUNRYYq+xYcWCvYslCVGxY+9drFiwi+WDiIhYUbCLiqAmgoIfFBVUsKDgF0G/KdgQ/wHPc529rOuemz3x3uS9kIGXc3Z3Znfe3Znfb/bAMhmFHTgOeBk4fRScGXUfVgBOGAEnbwDOKvw4Cliu6BtrXgb8AVxSG5wmfVcDfwLnDPT3HmDzgTZToV47sHWBu/sW+xTYuW9wmvR/DWwzwNcjgCsH6E+lau3AXO884KBy4dWAv4Dly4Fp1J4J/DbQ3w+BGQNthqqf1hnc0fPL97vvwFYE3iwXnQu8DZiCowAPpX8t7eOB14D1gE0bDDbrEOWNCfS2AvafYLw25AHssphI1XdgrvMWsGG+4P3A0wEnewIX5oNL6H0D4OCGX587DwGPAVvHRpcEXtqdAlxfdgKrAM4lPaw0gBMNgHu7fdwkNnd+Ze6+rquAD4AFwPkVpdsBK8lx+QTYL1o6/PP4yD8vws3eRd/iNC0Inu0x3Lfr74OOvH/lHvsvOpTYPcbWBH4s9HYCTs76bukpo0/MDtL5bstsJno16A8LBdcxICZLLgA81DHxgH7P+EvSLrlgB+DQZNDw/A/mhs22wHMN9kNV5OBfMiOz46es7essYOOsz+pQGC3lwAjYdwEPr1Xc0O+Dbwy+yZSTuvO5K01oZr2XGsC5i8D2TLX6Kn/IJTW5uCfl1d2oAQ6FzNq9xMgW55Nc2sH7S6kBrFHhlZuBMzOd/FVY9Q70VYEsWwAGh1kuV5WyR9zpzO61ysH/0T4buDHZS6yPpgbwMSB8KOt0uKoT8ptkXso+wOGRfWL20YHDbp7lqCKme887oPsz3wKzo798GP0tHJZXVmkOfXgwNYDPui8HZrNitm0JPBG+RPdY9pQcJow9lRSAa4Bjom1gC7XypNAuBO4YY/d1qHRFZvdCY+GTmUz4qp/jHGa0vBKRa7FxRmaaeO3VAk6SyuNdFB4LCCPbR6cbk7jErwgSqU/l83hO9sPSPGWUl/+cq7aLxd4vSniRwM9BuZwK7BUda0cQJN/TPK/HwecBLB+uH3buw63ZpAb1TdF2Lg93qHg+af4xWxuXA1Y6Nfmy1hkw9mJn+ytgGax4KAm2zJiUvWaWulMlQqr/IeeptJY8bWFVykdRCeb9Btu1EYjegUr5LjosxJL4f81yM8FnElHFq9I3sSe7ZvuRdBb11AevXM1ixDwf2kayGaVIsgn2rus4ZDfAP2HBIWwJJXKLJaniJsgt4vGSlkM67nmgsqhQ3VLNrR6fiKyU091N2xaxAHkyFP18ZhYPEdHCoqNZhJh0L5PTUkqL3x6eB5VgxHuL8CDGrxorCAfym7qW50m32YFJULwz46J8OiFqYRQSeX/5bgbNCT602DgyQ5RSt2x7SH7tUMwUkaBVDBRpp0ksef1DYva/btlN1qOh5AVYSHknLsQ1r+QiK+OpEusAOWhe9zX+h4GLXATIpU3ySESl5e10Fas8s92vKEtL0l1OhKnB8tLya9m6lR0QSp8Jrn847m8VtfauvwGpo54y5zFPkgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Regression\n",
    "Stepwise regression is used for fitting regression models with predictive models. It is carried out automatically. With each step, the variable is added or subtracted from the set of explanatory variables. The approaches for stepwise regression are forward selection, backward elimination, and bidirectional elimination. The formula for stepwise regression is\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import StepwiseLinearRegression\n",
    "\n",
    "# Create a stepwise regression model\n",
    "model = StepwiseLinearRegression(forward=True,backward=True,verbose=1)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression\n",
    "A Decision Tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. There is a non-parametric method used to model a decision tree to predict a continuous outcome.\n",
    "\n",
    "Here is the code for simple demonstration of the Decision Tree regression approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression\n",
    "Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees. \n",
    "\n",
    "Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. This part is called Bootstrap.\n",
    "\n",
    "Here is the code for simple demonstration of the Random Forest regression approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a random forest regression model\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)\n",
    "Support vector regression (SVR) is a type of support vector machine (SVM) that is used for regression tasks. It tries to find a function that best predicts the continuous output value for a given input value.\n",
    "\n",
    "SVR can use both linear and non-linear kernels. A linear kernel is a simple dot product between two input vectors, while a non-linear kernel is a more complex function that can capture more intricate patterns in the data. The choice of kernel depends on the data’s characteristics and the task’s complexity.\n",
    "\n",
    "Here is the code for simple demonstration of the Support vector regression approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create a support vector regression model\n",
    "model = SVR(kernel='linear')\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAANsAAAAcCAYAAADobspMAAAJUklEQVR4Ae2cVYxsRRCGf9zd3d3d3d3d3d3dgru7u7u7vJDwAISEECAhEEJC4IUACSFIf7tdm7p9z+mzMztypSuZPX3au6b/su5ZqVDhQO84MK+koyRtL+kDSTv2buiej7S/pG0lXSzpbUkT93wGZcDxmgPnRaDBhOUC2D4fR7kxtaR33Npel7SFey/JPnJgRTe2T7vssSq5gqSJ4oz9euaRNFvMZ/O91edVzS5pzjiHpSVN2uJ8JpBk65tW0kKu/Sou/aUkeDIKTSfpMkm3S7pZ0k6xdL9RapWXKg7M7zZYVXku7yZXeKNL9yK5uaRTo0mH5lkwDgoYTpa0tyRMoiraTtKmktZMNtr5kmaODarWM5Wk9yQtXNVpB/IA9V6SAEOOWKNpHNY6X03lBSJ/VpK0maszhaTL4/vKkVeueCCJ2YwpOQphP6P61nW56wdGviHpJZfXTnKpNqRGO+P0s81vkg5scwIINiOftrxdggS+w1668Fxe0n+SDnB9A4hPJa0XNsskLt+ShyYAe8IKwtODLV3PhJLuyWxs182Iko9IOqmhhy0lbRXrADYEZkoIBABjdIYk+AXBoytiGk2WKqVNJJ0Ty4ceMPT3hHlWuEcHwHaKJOzYcZmQfmY6tbpOvyF92vqZUtKM9tKl5/uSTAuhEY6WhGlURYslEp763v/Kge3YADYCJYtKOqGq84q81SRtXJGfy0Jjf5urIGk4YDs+6QOwIfygHNhQMAcHS3FyScxlSCPC6FtiB+mDKIqXWml50zum6XfjAdia+JAr9wDz6VybTpexgf6QNI2kY+JGqhsjNSsxJy9wlevAhqWE+Wifc12bXBKg7ZCrUFGGBv1Zkved0mpNYJte0jau0WRB078riT0N1YENzDzu1sl6Z6UBHWJCgMI6MicQCXaipN2irY6KtIFRrWtIWkvS7pJuC77fHMExfD6o0n+CGXlJ6JwvgfE6TUgP/A58zJ0T2xkT4CdJx0V/FGkH03CGmRNmwJuSkFj7RFv/CEk/StozCqG74jrmimbBVcGUeC5uTNZCiBeTizEg5sA4SG7q3hC+9JclzR3L04cHmE8vEuZ6aZDqH4V5ot26SWxO1ozgRfPkiPWtHtd2TfTxvX9UB7Zcn7mydsBGf8Qfrst03AS2raNmulDSw8H0vTdqZOuyDmxWPtoTxgK2fUcrGT3jzMQOxsb9MDqiTAbH1Og0S0QTFYmZI9Q1X1Luw6auIkD2iSv4ShKmjtEzYaMDGACGgGBjsRZzbrHLf0kcaiQTgSIj+rjbXoJz/1DCM5xgBJERQCbaZqYloPY8sXo8PcB8mjZISaJZFrjw7Tqdxs/5JuFDOgbfo5f2lCNQfSRvTAEbSgLtZt9BupYmsKFUPLH/DnEZLYMNrfC3JGzpOpolFvxaEcJEgq8aNcufku6UhGPoCX+wCWy+fqtpmAnQ8WswVT5O5gAwvPNP/0i9s+NAaA144DfMfUEjHuYmcmsCJjSW59lFSTmgvtK1xwE3cLvsgaQHmE9jLeCvfJY2cO9HhnROQDHucIiNxWaCD0QY64joXerLwZsNXIORgg3B79f0gKTHkrzUlHXDDySJERDl/cL7S0mlJrBhoXnCL3/NZbQMNtrSwf2ukzRJlAVt8FewRZdIClkMNwIgTFFU7Q/BvLw25vEAbBYgsXMWVzziJItGCyF1OOt4xYV06RywpTY/jHsqmrr4q4DPE1+wjy5hFmNeGmE++WhXCjb8EfKM2PRX20vy9ADzaaqhTb00TZp25BV/jdA1xLrR4nVESD0lotUIXKORgs36sWerZiR7DcuEvUZw7kHrKHnmwMYZ3IZJ/bUlPe3y2gIb5yQAws5GXH+aKVw3WSdmcAzgpR52Ov4Qzt/prhGL/dq9ExYnDw1U9WVRdThmZGq+2BAA3EeNMN+IAJmGrQIb5fiUmJBoj5TGBLChab+PmsRrjnSuI3lHmxHgMFpW0r+ZM7A0qIHVY66E9dFPsLHPuLGxZJwMFg+BH87EUsqBDa2Gz+wp1eBtgY0OOWd70Z3wk8dG3NWNhjZg42J6QqhygggQAQCTjrx7xxQHny8Rp9pLwMGWI//7aLDNMacgpBmaGm3AsQU+D5vh8FhuD+aDBDdzBTOTwAmEUCBQcFZ8R6u/GgMqZCFknk3OUPB3OAuzQAZg9dYCmpFPFXlt5tP4aQgONoodvla1bycPIYoP+ULSmAAWgRL2QhoowUx/0lkpBMdobxvbuuoX2Nj8fNcb2UTik/Nj26e+KAc2gl0W4qcNpm1qmbQNNjqcIZo+MAt/xYPHJgmDkYSodh+E4B1JwKE4m8QASTuuqqB5PHCtv0482eCAC3+NOQMazom4inNQGMAARbTRCEaiKWmDuQCw8PVYH4fT1oZy2tk72hKpZ+98YQgqeycai1lt75RxcGrvqdMdimoDJIAa09yiwdTtBOFz2Xx42pUl+sYPtTJulHgiEIUmw7qBL6zNotG+Hu3NSvLCw9dpJT1cMxLlYIfUvn/OvBC8KeXAhnBeJq6TetweSWlEYEs7G1ff0Xbe0bV1skn4YntNfkP6dK/n0TSe91lzdTsNNoS63drIjdtqWR3YuLZFWRMVsDVxKJYTsMB5RupB+ETXJxo5FnX94QHm010fuMUB+gW2Fqc57Op1YMOlwPdrogK2Jg65ciJOMBbTyZvErkpPknYEwWA+3ZPBWxgEN2M4hL9jRz1j8nq4pkiEEeKu58AtjxauxhHAssAgUXrM6kKFA4UDhQP95QAOMMGR9DylalZILoIC/oMZUahwoHCggQNETe0chatcFvbPNSP8SwiZUDz2PhFWzigLFQ4UDtRwgKMC+2EsVTiHIUIJcZbEnUSuY1kI3PwPwMYHIo8jEH5cWahwoHCghgPcHuHwGzBxf84OQTkb5JpZnXnowbZ4vHPXrbPDmqmX7MKBsYsDPmqEGcgBOMT9Sf4RTR0BNu5LAkhug3Brpe5GeV0fJb9wYLzigL+UzAEmP9OHuDkCkMx8tGdqRnJDhpsK/ufygz2Uv4UDhQNDHOC6kb8MzTmJ/WqB6z65/yNimo3OuHZFkMX/z5ahQUqicKBwYBBY/IqBC9FoJ6/l4A8gqrr0Sz1+nc1PLPi3ARDRSP5lhP26IGaXR+FA4QAc8FqtjiMEPwBSakbW1S/5hQOFAxUcKNHDCqaUrO5z4H+UzXrB4HaMlAAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Ridge regression is a technique for analyzing multiple regression data. When multicollinearity occurs, least squares estimates are unbiased. This is a regularized linear regression model, it tries to reduce the model complexity by adding a penalty term to the cost function. A degree of bias is added to the regression estimates, and as a result, ridge regression reduces the standard errors.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create a ridge regression model\n",
    "model = Ridge(alpha=0.1)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Lasso regression is a regression analysis method that performs both variable selection and regularization. Lasso regression uses soft thresholding. Lasso regression selects only a subset of the provided covariates for use in the final model.\n",
    "\n",
    "This is another regularized linear regression model, it works by adding a penalty term to the cost function, but it tends to zero out some features’ coefficients, which makes it useful for feature selection.\n",
    "\n",
    "Here is the code for simple demonstration of the Lasso regression approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create a lasso regression model\n",
    "model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression\n",
    "Linear Regression suffers from overfitting and can’t deal with collinear data. When there are many features in the dataset and even some of them are not relevant to the predictive model. This makes the model more complex with a too-inaccurate prediction on the test set (or overfitting). Such a model with high variance does not generalize on the new data. So, to deal with these issues, we include both L-2 and L-1 norm regularization to get the benefits of both Ridge and Lasso at the same time. The resultant model has better predictive power than Lasso. It performs feature selection and also makes the hypothesis simpler. The modified cost function for Elastic-Net Regression is given below:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "where,\n",
    "\n",
    "w(j) represents the weight for the jth feature. <br> \n",
    "n is the number of features in the dataset. <br>\n",
    "lambda1 is the regularization strength for the L1 norm. <br>\n",
    "lambda2 is the regularization strength for the L2 norm. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create an elastic net regression model\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Linear Regression\n",
    "As the name suggests this algorithm is purely based on Bayes Theorem. Because of this reason only we do not use the Least Square method to determine the coefficients of the regression model. So, the technique which is used here to find the model weights and parameters relies on features posterior distribution and this provides an extra stability factor to the regression model which is based on this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.linear_model import BayesianLinearRegression\n",
    "\n",
    "# Create a Bayesian linear regression model\n",
    "model = BayesianLinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the response for a new data point\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
